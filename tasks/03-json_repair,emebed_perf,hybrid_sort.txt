### Phase 1: Robustness (JSON Repair)
We need to set up the library first so the extraction logic can rely on it. Since you are using ES Modules in the browser without a bundler, we will fetch the ESM-compatible version of `@toolsycc/json-repair` from a CDN and save it locally.

**Instructions for Claude Code:**
1.  **Create Directory:** Create a new folder `src/lib`.
2.  **Download Library:** Download the ESM version of `@toolsycc/json-repair` and save it as `src/lib/json-repair.js`.
    *   *Command:* `curl -L "https://esm.sh/@toolsycc/json-repair?module" -o src/lib/json-repair.js`
    *   *Note:* If `curl` isn't available, instruct the agent to create the file and paste the source code.
3.  **Update Parser:** Modify `src/extraction/parser.js`.
    *   Import `repairJson` from `../lib/json-repair.js`.
    *   Update `parseExtractionResult`. Instead of `parseJsonFromMarkdown`, use `repairJson(jsonString, { returnObject: true, extractJson: true })`.
    *   Add error handling: If `repairJson` fails or returns `null`, return an empty array `[]` (but log the error).

### Phase 2: Performance (Parallel Embeddings)
Refactor the embedding generation to run in batches instead of one-by-one.

**Instructions for Claude Code:**
1.  **Modify:** `src/embeddings.js`.
2.  **Add Helper:** Create a `processInBatches` helper function at the top of the file (not exported).
    ```javascript
    async function processInBatches(items, batchSize, fn) {
        const results = [];
        for (let i = 0; i < items.length; i += batchSize) {
            const batch = items.slice(i, i + batchSize);
            const batchResults = await Promise.all(batch.map(fn));
            results.push(...batchResults);
        }
        return results;
    }
    ```
3.  **Refactor:** Rewrite `generateEmbeddingsForMemories`.
    *   Filter valid memories first (those with summaries and no embeddings).
    *   Use `processInBatches` with a batch size of **5** (safe for Ollama local instances).
    *   Map the batch to `getEmbedding(memory.summary)`.
    *   Assign results back to the memory objects.

### Phase 3: Extraction Context Bloat (Hybrid Sorting)
Change the logic of which memories are shown to the LLM during extraction to prioritize a mix of Recency and Importance.

**Instructions for Claude Code:**
1.  **Modify:** `src/extraction/extract.js`.
2.  **Locate:** The section inside `extractMemories` where `existingMemories` are selected using `sliceToTokenBudget`.
3.  **Implement Hybrid Selection Logic:**
    *   **Budget Split:** Calculate `totalBudget` (from settings). Allocate `25%` for Recency and `75%` for Importance.
    *   **Step A (Recency):** Sort all memories by Date/Sequence (Descending). Select the top memories that fit into the 25% budget. Keep track of which IDs were selected.
    *   **Step B (Importance):** From the *remaining* unselected memories, filter for `importance >= 4`. Sort these by Importance (Desc) or Date (Desc). Select as many as fit into the remaining 75% budget.
    *   **Step C (Fill):** (Optional but recommended) If space remains in the 75% bucket (e.g. not enough High Importance memories), fill the rest with the remaining recent memories to maximize context usage.
    *   **Step D (Merge):** Combine Step A and Step B lists.
    *   **Step E (Final Sort):** Sort the combined list by `sequence` (Ascending) so the LLM reads them in chronological order.
4.  **Pass:** Pass this new `finalContextMemories` list to `buildExtractionPrompt`.
