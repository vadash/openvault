TL;DR: Yes, `embeddinggemma-300m` has a structured prompt system via `task: X | query:` prefixes you can pass to `model.encode()`. For RP use, `STS` is your best built-in prompt, you can craft custom task strings, and BM25 hybrid is worth adding for genre/trope-specific vocabulary.

***

## Prompt Support

`google/embeddinggemma-300M` fully supports instructional prompts via the `prompt` or `prompt_name` argument in `model.encode()`. These steer the embedding space toward a specific task. Available built-in prompt names are:

| `prompt_name` | Prefix string |
|---|---|
| `STS` | `task: sentence similarity \| query: ` |
| `Retrieval-query` | `task: search result \| query: ` |
| `Retrieval-document` | `title: none \| text: ` |
| `Clustering` | `task: clustering \| query: ` |
| `Summarization` | `task: summarization \| query: ` |

Using a prompt consistently (both at index time and query time) significantly boosts score quality — the `STS` example raised similarity from `0.80` to `0.94` on matched sentences.

## RP-Specific Prompt Tuning

The model was trained on the `task: X | query:` format, so **custom task strings in that format are valid** . Since there's no RP-specific built-in, use the `prompt=` argument directly:

```python
# For comparing RP scenes/passages
rp_embedding = model.encode(
    text,
    prompt="task: narrative similarity | query: "
)

# For smut/erotic content matching
erotic_embedding = model.encode(
    text,
    prompt="task: romantic scene similarity | query: "
)
```

The model interprets these task descriptors to reshape its attention over thematic rather than purely semantic features. Use the **same custom prompt string for both your query and your indexed documents** — asymmetry degrades results.

For RP-specific tips:
- **`STS`** is the most stable built-in for scene-to-scene comparison (slice of life, emotional beats)
- **Custom `task:` strings** like `task: character relationship | query:` or `task: erotic tension | query:` can improve genre-specific matching since the model will focus on that framing
- **Asymmetric indexing** (store docs with `title: [scene type] | text:` and query with `task: narrative similarity | query:`) can help distinguish scene types

## BM25 Hybrid

Yes, add BM25 — especially for RP content. Dense embeddings miss exact keyword matches for:
- Character names and niche terms (fandom-specific, kink terminology, specific tropes)
- Short, context-dependent passages where semantic context is thin

A standard hybrid approach is reciprocal rank fusion (RRF):

```python
final_score = alpha * dense_score + (1 - alpha) * bm25_score
# alpha ~0.7 favors semantic; tune per corpus
```

Start at `alpha=0.7` and tune from there. If your corpus uses very domain-specific vocabulary (uncommon kink terms, character names), BM25 weight matters more — push `alpha` down to `0.5–0.6`.

## MRL Dimensions

The model supports truncated embeddings via [Matryoshka Representation Learning](pplx://action/navigate/ba9c159a96ffb0b8): 768 → 512 → 256 → 128 dims. For RP retrieval at scale, **512 dims** gives near-full quality at lower storage cost. Always use `normalize_embeddings=True` when truncating.

---

TL;DR: Don't manually curate every word — layer **stopwords → POS filter → domain glossary** and let BM25's IDF handle the rest automatically. The IDF component already penalizes corpus-common words.

***

## Why BM25 Self-Regulates (Partly)

BM25's IDF term already demotes words that appear in most of your documents (like "the", "she", "was") — they get near-zero weight automatically. So full-corpus BM25 isn't as noisy as it sounds. The real mess comes from **query-side** terms, not document-side.

***

## Practical Filtering Stack

Layer these in order:

1. **Standard stopwords** — remove function words (`is`, `a`, `the`, `and`). Pass as a set to the `stopwords=` param in most BM25 libs

2. **POS filtering** — keep only nouns (`NOUN`), proper nouns (`PROPN`), and verbs (`VERB`). Use spaCy:

```python
import spacy
nlp = spacy.load("en_core_web_sm")

def extract_bm25_terms(text):
    doc = nlp(text)
    keep = {"NOUN", "PROPN", "VERB"}
    return [t.lemma_.lower() for t in doc if t.pos_ in keep and not t.is_stop]
```

This catches character names, scene actions, and objects — the RP signal — while dropping filler.

3. **Domain glossary boost** — build a whitelist of RP-specific terms your embedding model is likely to blur together semantically (trope names, kink tags, fandom terms, character names). Always include these in BM25 terms regardless of POS. BM25 will score them highly because they're rare corpus-wide.

***

## For RP Specifically

| Term type | BM25 value | Strategy |
|---|---|---|
| Character names | Very high (corpus-rare) | Always include via `PROPN` |
| Kink/trope tags | High (domain-rare) | Whitelist |
| Scene verbs (`pinned`, `whispered`) | Medium | Keep via `VERB` filter |
| Emotional adjectives (`tender`, `rough`) | Low–Medium | Optional; let IDF decide |
| Common RP filler (`she`, `his`, `felt`) | Near-zero | Stopwords or IDF kills it |

***

## Keyphrase Extraction (Optional Upgrade)

If POS filtering feels too coarse, use **[KeyBERT](pplx://action/navigate/14b383a19bac947c)** to extract the top-N most distinctive phrases per passage before indexing:

```python
from keybert import KeyBERT
kw_model = KeyBERT()
keywords = kw_model.extract_keywords(text, top_n=8, stop_words="english")
bm25_tokens = [kw for kw, score in keywords]
```

This is especially useful for longer scenes where POS filtering might return 50+ terms. `top_n=6–10` is a good range for scene-length passages.